{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from dragonn.tutorial_utils import *\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am process:\n",
      "6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load keras libraries\n",
    "print ('I am process:')\n",
    "print (os.getpid())\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten, Activation, Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import ActivityRegularization\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load sklearn libraries for evaluation purposes\n",
    "from sklearn.utils import shuffle\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn import metrics as me\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts DNA sequence into one_hot_encoding format\n",
    "def get_training_input_data(input_file_path):\n",
    "    x_train = []\n",
    "    with open(input_file_path, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        reader.next()\n",
    "        for num, sequence in reader:\n",
    "            x_train.append(sequence)\n",
    "    return x_train\n",
    "\n",
    "def get_training_output_data(output_file_path):\n",
    "    y_train = []\n",
    "    with open(output_file_path, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        reader.next()\n",
    "        for label in reader:\n",
    "            y_train.append(label[1:])\n",
    "    return y_train\n",
    "\n",
    "def one_hot_encode(sequences):\n",
    "    sequence_length = len(sequences[0])\n",
    "    integer_type = np.int8 if sys.version_info[\n",
    "        0] == 2 else np.int32  # depends on Python version\n",
    "    integer_array = LabelEncoder().fit(np.array(('ACGTN',)).view(integer_type)).transform(\n",
    "        sequences.view(integer_type)).reshape(len(sequences), sequence_length)\n",
    "    one_hot_encoding = OneHotEncoder(\n",
    "        sparse=False, n_values=5, dtype=integer_type).fit_transform(integer_array)\n",
    "\n",
    "    return one_hot_encoding.reshape(\n",
    "        len(sequences), sequence_length, 5, 1).swapaxes(1, 2)[:, [0, 1, 2, 4], :, :]\n",
    "    #return one_hot_encoding.reshape(\n",
    "    #    len(sequences), 1, sequence_length, 5).swapaxes(2, 3)[:, :, [0, 1, 2, 4], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (75, 4, 51, 1)\n",
      "output shape: (75, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# call train_test_split\n",
    "# input_file_path = '/data/promoter_site/PromoterTrain.csv'\n",
    "# output_file_path = '/data/promoter_site/SigmaTrain.csv'\n",
    "input_file_path = '/data/promoter_site/smallTrainX.csv'\n",
    "output_file_path = '/data/promoter_site/smallTrainY.csv'\n",
    "x_train = get_training_input_data(input_file_path)\n",
    "#print x_train.shape\n",
    "y_train = get_training_output_data(output_file_path)\n",
    "#print y_train\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "x_train = one_hot_encode(np.array(x_train))\n",
    "print 'input shape: ' + str(x_train.shape)\n",
    "x_val = one_hot_encode(np.array(x_val))\n",
    "y_train = np.array(y_train).astype(bool)\n",
    "y_val = np.array(y_val).astype(bool)\n",
    "print 'output shape: ' + str(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (4, 3), activation=\"relu\", kernel_regularizer=<keras.reg..., input_shape=(4, 51, 1), bias_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (1, 15), activation=\"relu\", kernel_regularizer=<keras.reg..., input_shape=(4, 51, 1), bias_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 49, 5)\n",
      "(None, 1, 35, 5)\n",
      "(None, 1, 21, 5)\n",
      "(None, 1, 21, 5)\n",
      "(None, 105)\n",
      "(None, 5)\n",
      "(None, 5)\n",
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/20\n",
      "75/75 [==============================] - 1s 16ms/step - loss: 0.6571 - val_loss: 0.6439\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 0s 328us/step - loss: 0.6365 - val_loss: 0.6284\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 0s 326us/step - loss: 0.6175 - val_loss: 0.6075\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 0s 329us/step - loss: 0.5923 - val_loss: 0.5764\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 0s 328us/step - loss: 0.5678 - val_loss: 0.5270\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 0s 336us/step - loss: 0.5374 - val_loss: 0.4865\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 0s 359us/step - loss: 0.5129 - val_loss: 0.4731\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 0s 357us/step - loss: 0.5121 - val_loss: 0.4737\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 0s 351us/step - loss: 0.4990 - val_loss: 0.4700\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 0s 370us/step - loss: 0.4943 - val_loss: 0.4678\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 0s 346us/step - loss: 0.4943 - val_loss: 0.4661\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 0s 356us/step - loss: 0.4903 - val_loss: 0.4661\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 0s 350us/step - loss: 0.4808 - val_loss: 0.4600\n",
      "Epoch 14/20\n",
      "75/75 [==============================] - 0s 350us/step - loss: 0.4788 - val_loss: 0.4481\n",
      "Epoch 15/20\n",
      "75/75 [==============================] - 0s 374us/step - loss: 0.4756 - val_loss: 0.4437\n",
      "Epoch 16/20\n",
      "75/75 [==============================] - 0s 348us/step - loss: 0.4735 - val_loss: 0.4455\n",
      "Epoch 17/20\n",
      "75/75 [==============================] - 0s 344us/step - loss: 0.4684 - val_loss: 0.4425\n",
      "Epoch 18/20\n",
      "75/75 [==============================] - 0s 350us/step - loss: 0.4697 - val_loss: 0.4396\n",
      "Epoch 19/20\n",
      "75/75 [==============================] - 0s 353us/step - loss: 0.4599 - val_loss: 0.4406\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 0s 352us/step - loss: 0.4560 - val_loss: 0.4357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa750338310>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dragonn.models import SequenceDNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D, GlobalMaxPooling2D\n",
    "from keras import regularizers\n",
    "\n",
    "#model = SequenceDNN(seq_length=51, num_tasks=1, num_filters=[15],conv_width=[15], pool_width=15, dropout=0.1)\n",
    "#model.train(x_train, y_train, validation_data=(x_val, y_val))\n",
    "\n",
    "num_filters=(5, 5, 5)\n",
    "conv_width=(3, 15, 15)\n",
    "pool_width=15\n",
    "seq_length = 51\n",
    "model = Sequential()\n",
    "for i, (nb_filter, nb_col) in enumerate(zip(num_filters, conv_width,)):\n",
    "    conv_height = 4 if i == 0 else 1\n",
    "    model.add(Conv2D(nb_filter, conv_height, nb_col, \\\n",
    "                     activation='relu', input_shape=(4,seq_length,1), \\\n",
    "                    kernel_regularizer=regularizers.l2(0.01), \\\n",
    "                    bias_regularizer=regularizers.l2(0.01)))\n",
    "    # BatchNormalization doesn't help reduce loss\n",
    "    # model.add(BatchNormalization())\n",
    "    print model.output_shape\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Conv2D(15, 4, 3, activation='relu', input_shape=(4,51,1)))\n",
    "#print model.output_shape\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(MaxPooling2D(pool_size=(1, pool_width)))\n",
    "print model.output_shape\n",
    "model.add(Flatten())\n",
    "print model.output_shape\n",
    "model.add(Dense(output_dim=5))\n",
    "print model.output_shape\n",
    "model.add(Activation('softmax'))\n",
    "print model.output_shape\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=12, validation_data=(x_val, y_val))\n",
    "\n",
    "#valid_result = model.test(x_val, y_val)\n",
    "#print valid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (_, 1, 4, 51)\n",
    "nclass = 32 # Five different sigma factors, present or absent -> 2^5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "2\n",
      "(2,)\n",
      "LabelEncoder()\n",
      "(12,)\n",
      "[[2 4 1 0 1 1]\n",
      " [2 1 1 1 0 4]]\n",
      "[[0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1]]\n",
      "[[[[0 0 0 1 0 0]\n",
      "   [0 0 1 0 1 1]\n",
      "   [1 0 0 0 0 0]\n",
      "   [0 1 0 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 0 1 0]\n",
      "   [0 1 1 1 0 0]\n",
      "   [1 0 0 0 0 0]\n",
      "   [0 0 0 0 0 1]]]]\n",
      "(2, 1, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "sequences = ['GTCACC','GCCCAT']\n",
    "seq = np.array(sequences)\n",
    "print(np.array(seq[0]).shape)\n",
    "print(len(seq))\n",
    "print(seq.shape)\n",
    "result = one_hot_encode(np.array(sequences))\n",
    "print(LabelEncoder().fit(np.array(('ACGTN',)).view(np.int8)))\n",
    "integer_type = np.int8\n",
    "integer_array = LabelEncoder().fit(np.array(('ACGTN',)).view(integer_type)).transform(seq.view(integer_type))\n",
    "print(integer_array.shape)\n",
    "integer_array = integer_array.reshape(len(sequences), 6)\n",
    "print(integer_array)\n",
    "\n",
    "one_hot_encoding = OneHotEncoder(sparse=False, n_values=5, dtype=integer_type).fit_transform(integer_array)\n",
    "print(one_hot_encoding)\n",
    "\n",
    "new = one_hot_encoding.reshape(len(sequences), 1, 6, 5).swapaxes(2, 3)[:, :, [0, 1, 2, 4], :]\n",
    "print(new)\n",
    "print(new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0 0 0 1 0 0]\n",
      "   [0 0 1 0 1 1]\n",
      "   [1 0 0 0 0 0]\n",
      "   [0 1 0 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 0 1 0]\n",
      "   [0 1 1 1 0 0]\n",
      "   [1 0 0 0 0 0]\n",
      "   [0 0 0 0 0 1]]]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np, random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from dragonn.models import SequenceDNN\n",
    "from simdna.simulations import simulate_single_motif_detection\n",
    "from dragonn.utils import one_hot_encode, get_motif_scores, reverse_complement\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split  # sklearn >= 0.18\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split  # sklearn < 0.18\n",
    "import sys\n",
    "\n",
    "# Settings\n",
    "\n",
    "seq_length = 500\n",
    "num_sequences = 8000\n",
    "num_positives = 4000\n",
    "num_negatives = num_sequences - num_positives\n",
    "GC_fraction = 0.4\n",
    "test_fraction = 0.2\n",
    "validation_fraction = 0.2\n",
    "do_hyperparameter_search = False\n",
    "num_hyperparameter_trials = 50\n",
    "num_epochs = 100\n",
    "use_deep_CNN = False\n",
    "use_RNN = False\n",
    "\n",
    "print('Generating sequences...')\n",
    "\n",
    "sequences, labels, embeddings = simulate_single_motif_detection(\n",
    "    'SPI1_disc1', seq_length, num_positives, num_negatives, GC_fraction)\n",
    "\n",
    "print('One-hot encoding sequences...')\n",
    "\n",
    "encoded_sequences = one_hot_encode(sequences)\n",
    "\n",
    "print('Getting motif scores...')\n",
    "\n",
    "motif_scores = get_motif_scores(encoded_sequences, motif_names=['SPI1_disc1'])\n",
    "\n",
    "print('Partitioning data into training, validation and test sets...')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_sequences, labels, test_size=test_fraction)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_fraction)\n",
    "\n",
    "print('Adding reverse complements...')\n",
    "\n",
    "X_train = np.concatenate((X_train, reverse_complement(X_train)))\n",
    "y_train = np.concatenate((y_train, y_train))\n",
    "\n",
    "print('Randomly splitting data into training and test sets...')\n",
    "\n",
    "random_order = np.arange(len(X_train))\n",
    "np.random.shuffle(random_order)\n",
    "X_train = X_train[random_order]\n",
    "y_train = y_train[random_order]\n",
    "\n",
    "# Build and train model\n",
    "\n",
    "if not do_hyperparameter_search:\n",
    "    hyperparameters = {'seq_length': seq_length, 'use_RNN': use_RNN,\n",
    "                       'num_filters': (45,), 'pool_width': 25, 'conv_width': (10,),\n",
    "                       'L1': 0, 'dropout': 0.2, 'num_epochs': num_epochs}\n",
    "    if use_deep_CNN:\n",
    "        hyperparameters.update({'num_filters': (45, 50, 50), 'conv_width': (10, 8, 5)})\n",
    "    if use_RNN:\n",
    "        hyperparameters.update({'GRU_size': 35, 'TDD_size': 45})\n",
    "    model = SequenceDNN(**hyperparameters)\n",
    "    model.train(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                save_best_model_to_prefix='best_model')\n",
    "\n",
    "else:\n",
    "    print('Starting hyperparameter search...')\n",
    "    from dragonn.hyperparameter_search import HyperparameterSearcher, RandomSearch\n",
    "    fixed_hyperparameters = {'seq_length': seq_length, 'use_RNN': use_RNN, 'num_epochs': num_epochs}\n",
    "    grid = {'num_filters': ((5, 100),), 'pool_width': (5, 40),\n",
    "            'conv_width': ((6, 20),), 'dropout': (0, 0.5)}\n",
    "    if use_deep_CNN:\n",
    "        grid.update({'num_filters': ((5, 100), (5, 100), (5, 100)),\n",
    "                     'conv_width': ((6, 20), (6, 20), (6, 20))})\n",
    "    if use_RNN:\n",
    "        grid.update({'GRU_size': (10, 50), 'TDD_size': (20, 60)})\n",
    "\n",
    "    # Backend is RandomSearch; if using Python 2, can also specify MOESearch\n",
    "    # (requires separate installation)\n",
    "    searcher = HyperparameterSearcher(SequenceDNN, fixed_hyperparameters, grid, X_train, y_train,\n",
    "                                      validation_data=(X_valid, y_valid), backend=RandomSearch)\n",
    "    searcher.search(num_hyperparameter_trials)\n",
    "    print('Best hyperparameters: {}'.format(searcher.best_hyperparameters))\n",
    "    model = searcher.best_model\n",
    "\n",
    "# Test model\n",
    "\n",
    "print('Test results: {}'.format(model.test(X_test, y_test)))\n",
    "\n",
    "# Plot DeepLift and ISM scores for the first 10 test examples, and model architecture\n",
    "\n",
    "if sys.version[0] == 2:\n",
    "    model.plot_deeplift(X_test[:10], output_directory='deeplift_plots')\n",
    "model.plot_in_silico_mutagenesis(X_test[:10], output_directory='ISM_plots')\n",
    "model.plot_architecture(output_file='architecture_plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
